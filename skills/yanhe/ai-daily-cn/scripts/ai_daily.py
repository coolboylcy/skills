#!/usr/bin/env python3
"""
AI Daily - AI å¤§æ¨¡å‹æ—¥æŠ¥ç”Ÿæˆå™¨
è‡ªåŠ¨æŠ“å–ã€ç­›é€‰ã€æç‚¼ LLM/Agent é¢†åŸŸçƒ­ç‚¹ä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–ä¸­æ–‡ç®€æŠ¥

Usage:
    python ai_daily.py [--date YYYY-MM-DD] [--output-dir PATH] [--debug]
"""

import os
import sys
import json
import urllib.request
import ssl
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
import xml.etree.ElementTree as ET
from dataclasses import dataclass, field, asdict
import hashlib

# ============== é…ç½®åŠ è½½ ==============

def load_config() -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent.parent / "config" / "sources.json"
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

# ============== æ•°æ®ç»“æ„ ==============

@dataclass
class NewsItem:
    """æ–°é—»é¡¹"""
    title: str
    source: str
    url: str
    published: str
    summary: str = ""
    core_summary: str = ""  # 100 å­—å·¦å³æ ¸å¿ƒå†…å®¹
    category: str = "general"
    priority: int = 3
    content: str = ""
    raw_data: Dict = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'source': self.source,
            'url': self.url,
            'published': self.published,
            'summary': self.summary,
            'category': self.category,
            'priority': self.priority,
        }

@dataclass
class PaperItem:
    """è®ºæ–‡é¡¹"""
    title: str
    arxiv_id: str
    authors: List[str]
    abstract: str
    categories: List[str]
    pdf_url: str
    published: str
    priority: int = 3
    summary: str = ""
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'arxiv_id': self.arxiv_id,
            'authors': self.authors,
            'abstract': self.abstract,
            'categories': self.categories,
            'pdf_url': self.pdf_url,
            'published': self.published,
            'priority': self.priority,
            'summary': self.summary,
        }

@dataclass
class KOLPost:
    """KOL åŠ¨æ€"""
    author: str
    content: str
    url: str
    published: str
    priority: int = 3
    summary: str = ""
    
    def to_dict(self) -> Dict:
        return {
            'author': self.author,
            'content': self.content,
            'url': self.url,
            'published': self.published,
            'priority': self.priority,
            'summary': self.summary,
        }

@dataclass
class DailyReport:
    """æ—¥æŠ¥ç»“æ„"""
    date: str
    generated_at: str
    total_items: int = 0
    
    # ä¸‰ä¸ªæ¿å—ï¼ˆåˆå¹¶æ ¸å¿ƒå¤§äº‹ä»¶å’Œå®˜æ–¹æ›´æ–°ä¸ºæ–‡ç« ï¼‰
    articles: List[NewsItem] = field(default_factory=list)  # ç²¾é€‰æ–‡ç« ï¼ˆåˆå¹¶åçš„ï¼‰
    kol_insights: List[KOLPost] = field(default_factory=list)  # KOL è§‚ç‚¹
    papers: List[PaperItem] = field(default_factory=list)  # å¿…è¯»è®ºæ–‡
    
    def _generate_paper_summary(self, paper: PaperItem) -> str:
        """ä¸ºè®ºæ–‡ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
        abstract = paper.abstract.strip()
        
        if not abstract:
            return "æœ¬æ–‡ç ”ç©¶äº†ç›¸å…³é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚"
        
        # æ¸…ç† HTML æ ‡ç­¾
        import re
        abstract = re.sub(r'<[^>]+>', '', abstract)
        abstract = re.sub(r'\s+', ' ', abstract)
        
        # ç®€å•ç¿»è¯‘ï¼šæå–å…³é”®ä¿¡æ¯ç”Ÿæˆä¸­æ–‡æ‘˜è¦
        title_keywords = paper.title.lower()
        summary_parts = []
        
        # æ ¹æ®æ ‡é¢˜å…³é”®è¯ç”Ÿæˆé¢†åŸŸè¯´æ˜
        if 'agent' in title_keywords or 'agent' in abstract.lower():
            summary_parts.append('è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ™ºèƒ½ä½“æ–¹æ³•')
        elif 'reasoning' in title_keywords or 'reasoning' in abstract.lower():
            summary_parts.append('è¯¥ç ”ç©¶æ”¹è¿›äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›')
        elif 'benchmark' in title_keywords or 'evaluat' in abstract.lower():
            summary_parts.append('è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†')
        elif 'efficient' in title_keywords or 'efficient' in abstract.lower():
            summary_parts.append('è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•')
        elif 'train' in title_keywords or 'train' in abstract.lower():
            summary_parts.append('è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•')
        else:
            summary_parts.append('è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°æ–¹æ³•')
        
        # æ·»åŠ åº”ç”¨åœºæ™¯
        if 'math' in title_keywords or 'math' in abstract.lower():
            summary_parts.append('ç”¨äºæ•°å­¦é—®é¢˜æ±‚è§£')
        elif 'code' in title_keywords or 'code' in abstract.lower():
            summary_parts.append('ç”¨äºä»£ç ç”Ÿæˆå’Œç†è§£')
        elif 'vision' in title_keywords or 'image' in abstract.lower():
            summary_parts.append('ç”¨äºè§†è§‰ä»»åŠ¡')
        elif 'gui' in title_keywords:
            summary_parts.append('ç”¨äºå›¾å½¢ç•Œé¢äº¤äº’')
        
        # æ·»åŠ æ•ˆæœè¯´æ˜
        if 'improv' in abstract.lower():
            summary_parts.append('æ˜¾è‘—æå‡äº†æ€§èƒ½')
        elif 'efficient' in abstract.lower() or 'faster' in abstract.lower():
            summary_parts.append('å¤§å¹…æé«˜äº†æ•ˆç‡')
        
        summary = 'ï¼Œ'.join(summary_parts) + 'ã€‚'
        
        # é™åˆ¶é•¿åº¦
        if len(summary) > 80:
            summary = summary[:80] + '...'
        
        return summary
    
    def to_markdown(self) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        md = []
        md.append(f"# ğŸ“° AI å¤§æ¨¡å‹æ—¥æŠ¥ | {self.date}\n")
        md.append(f"*ç”Ÿæˆæ—¶é—´ï¼š{self.generated_at} | å…±æ”¶å½• {self.total_items} æ¡ä¿¡æ¯*\n")
        md.append("---\n")
        
        # 1. ç²¾é€‰æ–‡ç« ï¼ˆæŒ‰è´¨é‡æ’åºï¼Œæœ€å¤š 10 æ¡ï¼‰
        md.append(f"## ğŸ“° ç²¾é€‰æ–‡ç« ï¼ˆ{len(self.articles)}/10ï¼‰\n")
        if self.articles:
            sorted_articles = sorted(self.articles, key=lambda x: -x.priority)
            for i, item in enumerate(sorted_articles, 1):
                source_tag = f"ã€{item.source}ã€‘" if item.source in ['é‡å­ä½', 'æœºå™¨ä¹‹å¿ƒ'] else ""
                stars = "â­" * min(item.priority, 5)
                md.append(f"{i}. {source_tag} {item.title} {stars}\n")
                
                # ç”Ÿæˆ 100 å­—å·¦å³çš„æ ¸å¿ƒæ‘˜è¦
                summary_text = item.core_summary if hasattr(item, 'core_summary') and item.core_summary else (item.summary if item.summary else item.content[:300])
                if summary_text:
                    summary_text = summary_text.strip()
                    # æ¸…ç† HTML æ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼
                    import re
                    summary_text = re.sub(r'<[^>]+>', '', summary_text)
                    summary_text = re.sub(r'\s+', ' ', summary_text)
                    # æ§åˆ¶åœ¨ 80-120 å­—å·¦å³
                    if len(summary_text) > 120:
                        summary_text = summary_text[:120] + '...'
                    md.append(f"\n**ğŸ“ æ ¸å¿ƒå†…å®¹**ï¼š{summary_text}\n")
                
                md.append(f"\n[é˜…è¯»åŸæ–‡]({item.url})\n")
                md.append("")
        else:
            md.append("*ä»Šæ—¥æš‚æ— ç²¾é€‰æ–‡ç« *\n")
        
        md.append("---\n")
        
        # 2. KOL å‰æ²¿è§‚ç‚¹ï¼ˆæœ€å¤š 3 æ¡ï¼‰
        md.append(f"## ğŸ’¬ KOL è§‚ç‚¹ï¼ˆ{len(self.kol_insights)}/3ï¼‰\n")
        if self.kol_insights:
            for item in sorted(self.kol_insights, key=lambda x: -x.priority):
                stars = "â­" * min(item.priority, 5)
                md.append(f"**@{item.author}** {stars}\n")
                
                # ç®€çŸ­æ‘˜è¦
                content = item.content.strip()[:200]
                import re
                content = re.sub(r'<[^>]+>', '', content)
                content = re.sub(r'\s+', ' ', content)
                if len(content) > 150:
                    content = content[:150] + '...'
                md.append(f"\n**ğŸ“ æ‘˜è¦**ï¼š{content}\n")
                
                md.append(f"\n[æŸ¥çœ‹åŸå¸–]({item.url})\n")
                md.append("")
        else:
            md.append("*ä»Šæ—¥æš‚æ—  KOL è§‚ç‚¹*\n")
        
        md.append("---\n")
        
        # 3. å¿…è¯»ç¡¬æ ¸è®ºæ–‡ï¼ˆæœ€å¤š 3 ç¯‡ï¼‰
        md.append(f"## ğŸ“š æ¨èè®ºæ–‡ï¼ˆ{len(self.papers)}/3ï¼‰\n")
        if self.papers:
            for i, paper in enumerate(sorted(self.papers, key=lambda x: -x.priority), 1):
                stars = "â­" * min(paper.priority, 5)
                md.append(f"{i}. {paper.title} {stars}\n")
                md.append(f"**ä½œè€…**: {', '.join(paper.authors[:5])}{'...' if len(paper.authors) > 5 else ''}\n")
                md.append(f"**åˆ†ç±»**: {', '.join(paper.categories)}\n")
                
                # ç”Ÿæˆä¸­æ–‡æ‘˜è¦
                chinese_summary = self._generate_paper_summary(paper)
                md.append(f"\n**ğŸ“ æ‘˜è¦**ï¼š{chinese_summary}\n")
                
                md.append(f"\n[PDF]({paper.pdf_url}) | [arXiv](https://arxiv.org/abs/{paper.arxiv_id})\n")
                md.append("")
        else:
            md.append("*ä»Šæ—¥æš‚æ— æ¨èè®ºæ–‡*\n")
        
        md.append("---\n")
        md.append(f"*æœ¬æ—¥æŠ¥ç”± AI Daily Skill è‡ªåŠ¨ç”Ÿæˆ | æ•°æ®æºï¼šRSS + Tavily Search + arXiv*\n")
        
        return '\n'.join(md)

# ============== æ•°æ®æŠ“å– ==============

class DataFetcher:
    """æ•°æ®æŠ“å–å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE
        
        # è·å–ç¯å¢ƒå˜é‡
        self.tavily_api_key = os.environ.get('TAVILY_API_KEY', '')
        self.github_token = os.environ.get('GITHUB_TOKEN', '')
    
    def fetch_url(self, url: str, timeout: int = 30, use_browser_ua: bool = False) -> str:
        """é€šç”¨ URL æŠ“å–"""
        # æœºå™¨ä¹‹å¿ƒéœ€è¦ä½¿ç”¨çœŸå®æµè§ˆå™¨ UA
        if use_browser_ua or 'jiqizhixin' in url:
            ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        else:
            ua = 'Mozilla/5.0 (compatible; AIDaily/1.0; +https://example.com/bot)'
        
        req = urllib.request.Request(
            url,
            headers={
                'User-Agent': ua,
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }
        )
        
        try:
            with urllib.request.urlopen(req, context=self.ssl_context, timeout=timeout) as response:
                return response.read().decode('utf-8', errors='ignore')
        except Exception as e:
            print(f"[WARN] Failed to fetch {url}: {e}", file=sys.stderr)
            return ""
    
    def fetch_rss(self, url: str) -> List[NewsItem]:
        """è§£æ RSS/Atom Feed æˆ–ç½‘é¡µ"""
        # ç‰¹æ®Šå¤„ç†æœºå™¨ä¹‹å¿ƒï¼ˆCloudflare é˜²æŠ¤ï¼‰
        if 'jiqizhixin.com' in url:
            return self.fetch_jiqizhixin()
        
        xml_data = self.fetch_url(url)
        if not xml_data:
            return []
        
        items = []
        try:
            root = ET.fromstring(xml_data)
            
            # æ£€æµ‹ Feed ç±»å‹
            if root.tag == 'rss':
                channel = root.find('channel')
                if channel is None:
                    return []
                entries = channel.findall('item')
            elif 'Atom' in root.tag or root.tag == '{http://www.w3.org/2005/Atom}feed':
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                entries = root.findall('atom:entry', ns)
            else:
                entries = root.findall('item') or root.findall('entry')
            
            for entry in entries:
                # æå–å­—æ®µ
                title_elem = entry.find('title')
                title = title_elem.text.strip() if title_elem is not None and title_elem.text else ""
                
                link_elem = entry.find('link')
                if link_elem is not None:
                    link = link_elem.get('href', link_elem.text) if hasattr(link_elem, 'get') else (link_elem.text or "")
                else:
                    link = ""
                
                # å°è¯•å¤šä¸ªå­—æ®µè·å–å†…å®¹
                desc_elem = entry.find('description') or entry.find('summary') or entry.find('content') or entry.find('content/encoded')
                content = ""
                if desc_elem is not None:
                    if desc_elem.text:
                        content = self._clean_html(desc_elem.text)
                    # å°è¯• atom:content
                    if not content:
                        content_elem = entry.find('{http://purl.org/rss/1.0/modules/content/}encoded')
                        if content_elem is not None and content_elem.text:
                            content = self._clean_html(content_elem.text)
                
                # 36 æ°ªç‰¹æ®Šå¤„ç†ï¼šcontent å¯èƒ½åœ¨ CDATA ä¸­
                if not content:
                    import re
                    cdata_match = re.search(r'<!\[CDATA\[(.+?)\]\]>', str(ET.tostring(entry, encoding='unicode')))
                    if cdata_match:
                        content = self._clean_html(cdata_match.group(1))
                
                date_elem = entry.find('pubDate') or entry.find('published') or entry.find('updated')
                published = date_elem.text if date_elem is not None and date_elem.text else datetime.now().isoformat()
                
                if title and link:
                    items.append(NewsItem(
                        title=title,
                        source=url,
                        url=link,
                        published=published,
                        content=content,
                        category="rss"
                    ))
        
        except ET.ParseError as e:
            print(f"[WARN] RSS parse error for {url}: {e}", file=sys.stderr)
        
        return items
    
    def fetch_jiqizhixin(self) -> List[NewsItem]:
        """æŠ“å–æœºå™¨ä¹‹å¿ƒç½‘é¡µï¼ˆç»•è¿‡ Cloudflareï¼‰"""
        items = []
        try:
            # å°è¯•ç›´æ¥è®¿é—® API
            api_url = "https://www.jiqizhixin.com/api/articles?limit=20"
            data = self.fetch_url(api_url)
            if data:
                import json
                result = json.loads(data)
                articles = result.get('data', {}).get('articles', [])
                for article in articles[:20]:
                    title = article.get('title', '')
                    url = article.get('url', '')
                    summary = article.get('abstract', '')
                    published = article.get('published_at', datetime.now().isoformat())
                    
                    if title and url:
                        items.append(NewsItem(
                            title=title,
                            source='æœºå™¨ä¹‹å¿ƒ',
                            url=f"https://www.jiqizhixin.com{url}" if url.startswith('/') else url,
                            published=published,
                            content=summary,
                            category="media"
                        ))
        except Exception as e:
            print(f"[WARN] æœºå™¨ä¹‹å¿ƒæŠ“å–å¤±è´¥ï¼š{e}", file=sys.stderr)
        
        return items
    
    def _clean_html(self, html: str) -> str:
        """æ¸…ç† HTML æ ‡ç­¾"""
        if not html:
            return ""
        text = re.sub(r'<[^>]+>', '', html)
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&quot;', '"')
        return text.strip()
    
    def fetch_article_content(self, url: str) -> str:
        """æŠ“å–æ–‡ç« æ­£æ–‡å†…å®¹"""
        html = self.fetch_url(url, timeout=15)
        if not html:
            return ""
        
        # ç®€å•æå–æ­£æ–‡ï¼ˆç§»é™¤ scriptã€style ç­‰ï¼‰
        text = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<nav[^>]*>.*?</nav>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<header[^>]*>.*?</header>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<footer[^>]*>.*?</footer>', '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # æå–æ®µè½
        paragraphs = re.findall(r'<p[^>]*>(.*?)</p>', text, flags=re.DOTALL | re.IGNORECASE)
        if paragraphs:
            # å–å‰ 3 æ®µä½œä¸ºæ‘˜è¦ç´ æ
            content = ' '.join([self._clean_html(p) for p in paragraphs[:3]])
            return content
        
        # å¦‚æœæ²¡æœ‰æ®µè½ï¼Œè¿”å›æ¸…ç†åçš„å…¨æ–‡ï¼ˆæˆªå–å‰ 500 å­—ï¼‰
        return self._clean_html(text)[:500]
    
    def fetch_tavily(self, query: str, max_results: int = 10) -> List[KOLPost]:
        """ä½¿ç”¨ Tavily Search æœç´¢"""
        if not self.tavily_api_key:
            print("[WARN] TAVILY_API_KEY not set, skipping Tavily search", file=sys.stderr)
            return []
        
        url = "https://api.tavily.com/search"
        payload = {
            "query": query,
            "max_results": max_results,
            "include_answer": False,
            "search_depth": "basic",
            "time_range": "day"
        }
        
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(
            url,
            data=data,
            headers={
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self.tavily_api_key}'
            }
        )
        
        try:
            with urllib.request.urlopen(req, context=self.ssl_context, timeout=30) as response:
                result = json.loads(response.read().decode('utf-8'))
                
            posts = []
            for item in result.get('results', []):
                try:
                    posts.append(KOLPost(
                        author="Unknown",
                        content=self._clean_html(item.get('content', '')),
                        url=item.get('url', ''),
                        published=datetime.now().isoformat()
                    ))
                except Exception as e:
                    if debug_mode:
                        print(f"[WARN] Failed to create KOLPost: {e}", file=sys.stderr)
            
            return posts
        except Exception as e:
            print(f"[WARN] Tavily search failed: {e}", file=sys.stderr)
            return []
    
    def fetch_arxiv(self, categories: List[str], keywords: List[str], max_results: int = 20) -> List[PaperItem]:
        """ä» arXiv è·å–è®ºæ–‡"""
        papers = []
        
        for category in categories:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            keyword_query = ' OR '.join([f'all:"{kw}"' for kw in keywords[:5]])  # é™åˆ¶å…³é”®è¯æ•°é‡
            search_query = f"cat:{category} AND ({keyword_query})"
            
            url = f"http://export.arxiv.org/api/query?search_query={urllib.parse.quote(search_query)}&start=0&max_results={max_results//len(categories)}&sortBy=submittedDate&sortOrder=descending"
            
            xml_data = self.fetch_url(url)
            if not xml_data:
                continue
            
            try:
                root = ET.fromstring(xml_data)
                ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}
                
                for entry in root.findall('atom:entry', ns):
                    title_elem = entry.find('atom:title', ns)
                    title = title_elem.text.strip() if title_elem is not None else ""
                    
                    summary_elem = entry.find('atom:summary', ns)
                    abstract = summary_elem.text.strip() if summary_elem is not None else ""
                    
                    id_elem = entry.find('atom:id', ns)
                    arxiv_id = id_elem.text.split('/')[-1] if id_elem is not None else ""
                    
                    published_elem = entry.find('atom:published', ns)
                    published = published_elem.text if published_elem is not None else ""
                    
                    # è·å–ä½œè€…
                    authors = []
                    for author in entry.findall('atom:author', ns):
                        name_elem = author.find('atom:name', ns)
                        if name_elem is not None:
                            authors.append(name_elem.text)
                    
                    # è·å–åˆ†ç±»
                    cats = []
                    for cat in entry.findall('atom:category', ns):
                        term = cat.get('term', '')
                        if term:
                            cats.append(term)
                    
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    
                    if title and arxiv_id:
                        papers.append(PaperItem(
                            title=title,
                            arxiv_id=arxiv_id,
                            authors=authors,
                            abstract=abstract,
                            categories=cats,
                            pdf_url=pdf_url,
                            published=published
                        ))
            
            except ET.ParseError as e:
                print(f"[WARN] arXiv parse error: {e}", file=sys.stderr)
        
        return papers

# ============== LLM å¤„ç† ==============

class LLMProcessor:
    """LLM å†…å®¹å¤„ç†å™¨"""
    
    def __init__(self, model: str = "qwen3.5-plus"):
        self.model = model
        self.api_key = os.environ.get('ALIBABA_CLOUD_API_KEY', '')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    
    def filter_noise(self, items: List[NewsItem], min_priority: int = 3) -> List[NewsItem]:
        """ä½¿ç”¨ LLM è¿‡æ»¤ä½è´¨é‡å†…å®¹"""
        # ç®€å•è§„åˆ™è¿‡æ»¤ï¼ˆå®é™…åº”è¯¥è°ƒç”¨ LLMï¼‰
        exclude_keywords = ['sponsor', 'advertisement', 'webinar', 'promo', 'å¹¿å‘Š', 'èµåŠ©']
        
        filtered = []
        for item in items:
            text = (item.title + ' ' + item.content).lower()
            
            # æ£€æŸ¥æ’é™¤å…³é”®è¯
            if any(kw.lower() in text for kw in exclude_keywords):
                continue
            
            # å¦‚æœ content ä¸ºç©ºä½†æœ‰æ ‡é¢˜ï¼Œä¹Ÿä¿ç•™ï¼ˆå¾ˆå¤š RSS åªè¿”å›æ ‡é¢˜ï¼‰
            if len(item.content) < 10 and len(item.title) < 10:
                continue
            
            # åˆæ­¥è¯„åˆ†
            priority = self._calculate_priority(item)
            if priority >= min_priority:
                item.priority = priority
                filtered.append(item)
        
        return filtered
    
    def _calculate_priority(self, item: NewsItem) -> int:
        """è®¡ç®—å†…å®¹ä¼˜å…ˆçº§ï¼ˆ1-10ï¼‰"""
        priority = 5  # åŸºç¡€åˆ†
        
        # å…³é”®è¯åŠ åˆ†ï¼ˆæ¯é¡¹ +1ï¼Œæœ€å¤šåŠ  3 åˆ†ï¼‰
        hot_keywords = [
            'GPT', 'Claude', 'Gemini', 'LLaMA', 'Qwen', 'é€šä¹‰åƒé—®',
            'transformer', 'agent', 'reasoning', 'MoE', 'æ··åˆä¸“å®¶',
            'fine-tuning', 'RLHF', 'RAG', 'prompt', 'æç¤ºè¯',
            'å¤§æ¨¡å‹', 'è¯­è¨€æ¨¡å‹', 'å¤šæ¨¡æ€', 'Agent', 'æ™ºèƒ½ä½“'
        ]
        
        text = (item.title + ' ' + item.content).lower()
        keyword_score = 0
        for kw in hot_keywords:
            if kw.lower() in text:
                keyword_score += 1
                if keyword_score >= 3:
                    break
        priority += keyword_score
        
        # ä¸­æ–‡åª’ä½“åŠ åˆ†ï¼ˆé‡å­ä½ã€æœºå™¨ä¹‹å¿ƒå·²ç»æ˜¯é«˜ä¼˜å…ˆçº§ï¼Œè¿™é‡Œä¸å†é‡å¤ï¼‰
        if item.source in ['é‡å­ä½', 'æœºå™¨ä¹‹å¿ƒ']:
            priority += 1
        
        # æ ‡é¢˜é•¿åº¦é€‚ä¸­åŠ åˆ†ï¼ˆå¤ªçŸ­å¯èƒ½æ˜¯æ ‡é¢˜å…šï¼Œå¤ªé•¿å¯èƒ½ä¸å¤Ÿç²¾ç‚¼ï¼‰
        if 20 <= len(item.title) <= 60:
            priority += 1
        
        # æœ‰æ‘˜è¦/å†…å®¹åŠ åˆ†
        if len(item.content) > 100 or len(item.summary) > 50:
            priority += 1
        
        return min(priority, 10)
    
    def summarize(self, item: NewsItem) -> str:
        """ç”Ÿæˆç®€çŸ­æ‘˜è¦"""
        # ä¼˜å…ˆä½¿ç”¨å·²æœ‰æ‘˜è¦
        if item.summary:
            return item.summary
        
        # ä»å†…å®¹æå–
        content = item.content[:500] if item.content else ""
        
        if content:
            # å–å‰ 2-3 å¥ä½œä¸ºæ‘˜è¦
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
            summary = '. '.join([s.strip() for s in sentences[:2] if s.strip()])
            return summary if summary else content[:200]
        
        # ä»æ ‡é¢˜ç”Ÿæˆæœ‰æ„ä¹‰çš„æ‘˜è¦
        if item.title:
            title = item.title
            # æå–å…³é”®è¯
            keywords = []
            if any(kw in title for kw in ['å¤§æ¨¡å‹', 'LLM', 'GPT', 'Claude', 'Qwen']):
                keywords.append('å¤§æ¨¡å‹æŠ€æœ¯')
            if any(kw in title for kw in ['èèµ„', 'æŠ•èµ„', 'æ”¶è´­']):
                keywords.append('æŠ•èèµ„åŠ¨æ€')
            if any(kw in title for kw in ['å‘å¸ƒ', 'ä¸Šçº¿', 'å¼€æº']):
                keywords.append('äº§å“å‘å¸ƒ')
            if any(kw in title for kw in ['Karpathy', 'LeCun', 'Hinton']):
                keywords.append('ä¸“å®¶è§‚ç‚¹')
            if any(kw in title for kw in ['è‹±ä¼Ÿè¾¾', 'NVIDIA', 'GPU', 'èŠ¯ç‰‡']):
                keywords.append('ç¡¬ä»¶åŠ¨æ€')
            
            if keywords:
                return f"æœ¬æ–‡æŠ¥é“äº†{item.source}å…³äº{'ã€'.join(keywords)}çš„æœ€æ–°åŠ¨æ€ã€‚"
            else:
                return f"æœ¬æ–‡æŠ¥é“äº†{item.source}çš„æœ€æ–° AI è¡Œä¸šåŠ¨æ€ã€‚"
        
        return ""
    
    def rate_paper(self, paper: PaperItem) -> int:
        """è¯„ä¼°è®ºæ–‡é‡è¦æ€§ï¼ˆ1-10ï¼‰"""
        priority = 5  # åŸºç¡€åˆ†
        
        # å…³é”®è¯åŒ¹é…ï¼ˆæ¯é¡¹ +1ï¼Œæœ€å¤šåŠ  3 åˆ†ï¼‰
        hot_keywords = [
            'large language model', 'LLM', 'agent', 'reasoning', 'MoE',
            'mixture of experts', 'transformer', 'multimodal', 'vision-language',
            'å¤§æ¨¡å‹', 'è¯­è¨€æ¨¡å‹', 'æ™ºèƒ½ä½“', 'æ¨ç†', 'å¤šæ¨¡æ€'
        ]
        text = (paper.title + ' ' + paper.abstract).lower()
        
        keyword_score = 0
        for kw in hot_keywords:
            if kw in text:
                keyword_score += 1
                if keyword_score >= 3:
                    break
        priority += keyword_score
        
        # å¤šä½œè€…åŠ åˆ†ï¼ˆåˆä½œç ”ç©¶é€šå¸¸è´¨é‡æ›´é«˜ï¼‰
        if len(paper.authors) > 5:
            priority += 1
        if len(paper.authors) > 10:
            priority += 1
        
        # çŸ¥åæœºæ„åŠ åˆ†
        prestigious = ['MIT', 'Stanford', 'Berkeley', 'Google', 'Meta', 'OpenAI', 'Microsoft', 'CMU']
        for author in paper.authors:
            if any(inst in author for inst in prestigious):
                priority += 1
                break
        
        return min(priority, 10)

# ============== ä¸»æµç¨‹ ==============

def generate_daily_report(date: Optional[str] = None, output_dir: Optional[str] = None, debug: bool = False):
    global debug_mode
    debug_mode = debug
    """ç”Ÿæˆæ—¥æŠ¥ä¸»å‡½æ•°"""
    
    # è®¾ç½®æ—¥æœŸ
    if date is None:
        date = datetime.now().strftime('%Y-%m-%d')
    
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ AI æ—¥æŠ¥ | æ—¥æœŸï¼š{date}")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = Path(__file__).parent.parent / "output"
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–
    fetcher = DataFetcher(config)
    processor = LLMProcessor()
    report = DailyReport(date=date, generated_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    
    # 1. æŠ“å– RSS Feed
    print("ğŸ“¡ æŠ“å– RSS Feed...")
    all_rss_items = []
    
    for feed_group in ['official', 'media']:
        feeds = config['rssFeeds'].get(feed_group, [])
        for feed in feeds:
            if debug:
                print(f"  - {feed['name']}: {feed['url']}")
            items = fetcher.fetch_rss(feed['url'])
            for item in items:
                item.source = feed['name']
                item.category = feed_group
                # æ ¹æ®é…ç½®è®¾ç½®ä¼˜å…ˆçº§
                item.priority = feed.get('priority', 3)
            all_rss_items.extend(items)
    
    print(f"  âœ“ è·å– {len(all_rss_items)} æ¡ RSS å†…å®¹")
    
    # è¿‡æ»¤å’Œæ€»ç»“
    filtered_rss = processor.filter_noise(all_rss_items, config['filter']['minPriority'])
    for item in filtered_rss:
        if not item.summary:
            item.summary = processor.summarize(item)
    
    # ä¸ºé«˜ä¼˜å…ˆçº§æ–‡ç« æŠ“å–ç½‘é¡µå†…å®¹å¹¶ç”Ÿæˆæ ¸å¿ƒæ‘˜è¦ï¼ˆ100 å­—å·¦å³ï¼‰
    print("ğŸ“ ç”Ÿæˆæ ¸å¿ƒæ‘˜è¦...")
    sorted_for_fetch = sorted(filtered_rss, key=lambda x: -x.priority)[:10]
    for i, item in enumerate(sorted_for_fetch, 1):
        if item.priority >= 7:  # åªä¸ºé«˜ä¼˜å…ˆçº§æ–‡ç« æŠ“å–
            print(f"  [{i}/10] æŠ“å–ï¼š{item.title[:50]}...")
            try:
                content = fetcher.fetch_article_content(item.url)
                if content:
                    # æå–å…³é”®ä¿¡æ¯ç”Ÿæˆ 100 å­—å·¦å³æ‘˜è¦
                    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
                    core_parts = []
                    char_count = 0
                    for sent in sentences:
                        sent = sent.strip()
                        if len(sent) > 10 and char_count < 120:
                            core_parts.append(sent)
                            char_count += len(sent)
                    item.core_summary = 'ã€‚'.join(core_parts) + 'ã€‚' if core_parts else item.summary
            except Exception as e:
                if debug:
                    print(f"  [WARN] æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{e}")
    
    # æå‡é‡å­ä½å’Œæœºå™¨ä¹‹å¿ƒçš„ä¼˜å…ˆçº§
    for item in filtered_rss:
        if item.source in ['é‡å­ä½', 'æœºå™¨ä¹‹å¿ƒ']:
            item.priority = min(item.priority + 2, 10)  # é¢å¤– +2 ä¼˜å…ˆçº§ï¼Œæœ€é«˜ 10
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    sorted_items = sorted(filtered_rss, key=lambda x: -x.priority)
    
    # å®æ–½æ¥æºé…é¢ï¼šé‡å­ä½æœ€å¤š 80%ï¼ˆ8 æ¡/10 æ¡ï¼‰
    max_quantum = 8  # é‡å­ä½æœ€å¤š 8 æ¡
    quantum_count = 0
    selected_items = []
    
    # ç¬¬ä¸€è½®ï¼šé€‰æ‹©éé‡å­ä½çš„é«˜ä¼˜å…ˆçº§æ–‡ç« ï¼ˆè‡³å°‘ 2 æ¡ï¼‰
    for item in sorted_items:
        if item.source != 'é‡å­ä½' and len(selected_items) < 10 - max_quantum:
            selected_items.append(item)
    
    # ç¬¬äºŒè½®ï¼šé€‰æ‹©é‡å­ä½æ–‡ç« ï¼ˆæœ€å¤š 8 æ¡ï¼‰
    for item in sorted_items:
        if item.source == 'é‡å­ä½' and quantum_count < max_quantum:
            selected_items.append(item)
            quantum_count += 1
    
    # ç¬¬ä¸‰è½®ï¼šå¦‚æœè¿˜æœ‰ç©ºä½ï¼Œè¡¥å……å…¶ä»–é«˜ä¼˜å…ˆçº§æ–‡ç« 
    for item in sorted_items:
        if len(selected_items) >= 10:
            break
        if item not in selected_items:
            selected_items.append(item)
    
    # æœ€ç»ˆé™åˆ¶ 10 æ¡
    report.articles = selected_items[:10]
    
    # 2. æŠ“å– KOL åŠ¨æ€ï¼ˆTavilyï¼‰
    print("ğŸ¦ æŠ“å– KOL åŠ¨æ€...")
    all_kol_posts = []
    
    if fetcher.tavily_api_key:
        for kol in config['tavilySearch']['kolQueries']:
            if debug:
                print(f"  - {kol['name']}: {kol['query']}")
            try:
                posts = fetcher.fetch_tavily(
                    kol['query'],
                    config['tavilySearch']['maxResults']
                )
                for post in posts:
                    post.author = kol['name']
                    post.priority = kol.get('priority', 3)
                all_kol_posts.extend(posts)
            except Exception as e:
                if debug:
                    print(f"  [WARN] {kol['name']} å¤±è´¥ï¼š{e}")
        
        print(f"  âœ“ è·å– {len(all_kol_posts)} æ¡ KOL åŠ¨æ€")
    else:
        print("  âš  æœªé…ç½® TAVILY_API_KEYï¼Œè·³è¿‡ KOL åŠ¨æ€æŠ“å–")
    
    report.kol_insights = all_kol_posts[:3]  # é™åˆ¶æœ€å¤š 3 æ¡
    
    # 3. æŠ“å– arXiv è®ºæ–‡
    print("ğŸ“š æŠ“å– arXiv è®ºæ–‡...")
    papers = fetcher.fetch_arxiv(
        config['arxiv']['categories'],
        config['arxiv']['keywords'],
        config['arxiv']['maxResults']
    )
    
    print(f"  âœ“ è·å– {len(papers)} ç¯‡è®ºæ–‡")
    
    # è¯„ä¼°è®ºæ–‡
    for paper in papers:
        paper.priority = processor.rate_paper(paper)
    
    report.papers = sorted(papers, key=lambda x: -x.priority)[:3]  # é™åˆ¶æœ€å¤š 3 ç¯‡
    
    # ç»Ÿè®¡
    report.total_items = (
        len(report.articles) +
        len(report.kol_insights) +
        len(report.papers)
    )
    
    # ç”Ÿæˆ Markdown
    print("ğŸ“ ç”ŸæˆæŠ¥å‘Š...")
    markdown = report.to_markdown()
    
    # ä¿å­˜æ–‡ä»¶
    filename = f"AI-Daily-{date}.md"
    output_file = output_path / filename
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown)
    
    print(f"âœ… æ—¥æŠ¥å·²ä¿å­˜è‡³ï¼š{output_file}")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼šç²¾é€‰æ–‡ç«  {len(report.articles)} | KOL è§‚ç‚¹ {len(report.kol_insights)} | è®ºæ–‡ {len(report.papers)}")
    
    return output_file

# ============== CLI ==============

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='AI Daily - ç”Ÿæˆå¤§æ¨¡å‹æ—¥æŠ¥')
    parser.add_argument('--date', type=str, help='æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©')
    parser.add_argument('--output-dir', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    try:
        output_file = generate_daily_report(
            date=args.date,
            output_dir=args.output_dir,
            debug=args.debug
        )
        
        # è¾“å‡ºåˆ° stdoutï¼ˆæ–¹ä¾¿ OpenClaw è¯»å–ï¼‰
        print("\n" + "="*60)
        print("ğŸ“° ä»Šæ—¥ç®€æŠ¥é¢„è§ˆ:")
        print("="*60)
        
        with open(output_file, 'r', encoding='utf-8') as f:
            # åªæ˜¾ç¤ºå‰ 50 è¡Œ
            lines = f.readlines()[:50]
            print(''.join(lines))
            if len(lines) == 50:
                print("\n... (å®Œæ•´æŠ¥å‘Šè§è¾“å‡ºæ–‡ä»¶)")
    
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{e}", file=sys.stderr)
        if args.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
